{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import src.utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_dataset_path': 'data/raw/data.csv',\n",
       " 'data_set_path': 'data/output/data.pkl',\n",
       " 'input_set_path': 'data/output/input.pkl',\n",
       " 'output_set_path': 'data/output/output.pkl',\n",
       " 'input_columns_path': 'data/output/input_columns.pkl',\n",
       " 'train_set_path': ['data/output/X_train.pkl', 'data/output/y_train.pkl'],\n",
       " 'valid_set_path': ['data/output/X_valid.pkl', 'data/output/y_valid.pkl'],\n",
       " 'test_set_path': ['data/output/X_test.pkl', 'data/output/y_test.pkl'],\n",
       " 'index_column': 'Unnamed: 0',\n",
       " 'output_column': 'SeriousDlqin2yrs',\n",
       " 'seed': 42,\n",
       " 'test_size': 0.2,\n",
       " 'clean_late_col': 'NumberOfTimes90DaysLate',\n",
       " 'clean_late_val': 96,\n",
       " 'clean_unsecure_col': 'RevolvingUtilizationOfUnsecuredLines',\n",
       " 'constant_imputer_col': 'NumberOfDependents',\n",
       " 'constant_imputer_path': 'data/output/constant_imputer.pkl',\n",
       " 'constant_imputer_val': 0.0,\n",
       " 'median_imputer_col': 'MonthlyIncome',\n",
       " 'median_imputer_path': 'data/output/median_imputer.pkl',\n",
       " 'standardizer_path': 'data/output/standardizer.pkl',\n",
       " 'preprocessor_path': 'data/output/preprocessor.pkl',\n",
       " 'train_clean_path': ['data/output/X_train_clean.pkl',\n",
       "  'data/output/y_train_clean.pkl'],\n",
       " 'valid_clean_path': ['data/output/X_valid_clean.pkl',\n",
       "  'data/output/y_valid_clean.pkl'],\n",
       " 'test_clean_path': ['data/output/X_test_clean.pkl',\n",
       "  'data/output/y_test_clean.pkl'],\n",
       " 'print_debug': True,\n",
       " 'list_of_model_path': 'log/list_of_model.pkl',\n",
       " 'list_of_param_path': 'log/list_of_param.pkl',\n",
       " 'list_of_tuned_model_path': 'log/list_of_tuned_model.pkl',\n",
       " 'best_model_path': 'models/best_model.pkl',\n",
       " 'best_threshold_path': 'models/best_threshold.pkl'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_DATA = utils.config_load()\n",
    "CONFIG_DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model of Choice\n",
    "- KNN\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_param():\n",
    "    \"\"\"Create the model objects\"\"\"\n",
    "    lgr_params = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1],\n",
    "        'max_iter': [100, 300, 500]\n",
    "    }\n",
    "\n",
    "    xgb_params = {\n",
    "        'n_estimators': [50, 100, 200]\n",
    "    }\n",
    "\n",
    "    # Create model params\n",
    "    list_of_param = {\n",
    "        'LogisticRegression': lgr_params,\n",
    "        'XGBClassifier': xgb_params\n",
    "    }\n",
    "\n",
    "    return list_of_param\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_object():\n",
    "    \"\"\"Create the model objects\"\"\"\n",
    "    print(\"Creating model objects\")\n",
    "\n",
    "    # Create model objects\n",
    "    lgr = LogisticRegression()\n",
    "    xgb = XGBClassifier()\n",
    "\n",
    "    # Create list of model\n",
    "    list_of_model = [\n",
    "        {'model_name': lgr.__class__.__name__, 'model_object': lgr},\n",
    "        {'model_name': xgb.__class__.__name__, 'model_object': xgb}\n",
    "    ]\n",
    "\n",
    "    return list_of_model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(return_file=True):\n",
    "    \"\"\"Function to get the best model\"\"\"\n",
    "    # Load dataset\n",
    "    X_train = utils.pickle_load(CONFIG_DATA['train_clean_path'][0])\n",
    "    y_train = utils.pickle_load(CONFIG_DATA['train_clean_path'][1])\n",
    "    X_valid = utils.pickle_load(CONFIG_DATA['valid_clean_path'][0])\n",
    "    y_valid = utils.pickle_load(CONFIG_DATA['valid_clean_path'][1])\n",
    "    \n",
    "    # Create list of params & models\n",
    "    list_of_param = create_model_param()\n",
    "    list_of_model = create_model_object()\n",
    "\n",
    "    # List of trained model\n",
    "    list_of_tuned_model = {}\n",
    "\n",
    "    # Train model\n",
    "    for base_model in list_of_model:\n",
    "        # Current condition\n",
    "        model_name = base_model['model_name']\n",
    "        model_obj = copy.deepcopy(base_model['model_object'])\n",
    "        model_param = list_of_param[model_name]\n",
    "\n",
    "        # Debug message\n",
    "        print('Training model :', model_name)\n",
    "\n",
    "        # Create model object\n",
    "        model = RandomizedSearchCV(estimator = model_obj,\n",
    "                                   param_distributions = model_param,\n",
    "                                   n_iter=5,\n",
    "                                   cv = 5,\n",
    "                                   random_state = 123,\n",
    "                                   n_jobs=1,\n",
    "                                   verbose=10,\n",
    "                                   scoring = 'roc_auc')\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        y_pred_proba_train = model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_proba_valid = model.predict_proba(X_valid)[:, 1]\n",
    "        \n",
    "        # Get score\n",
    "        train_score = roc_auc_score(y_train, y_pred_proba_train)\n",
    "        valid_score = roc_auc_score(y_valid, y_pred_proba_valid)\n",
    "\n",
    "        # Append\n",
    "        list_of_tuned_model[model_name] = {\n",
    "            'model': model,\n",
    "            'train_auc': train_score,\n",
    "            'valid_auc': valid_score,\n",
    "            'best_params': model.best_params_\n",
    "        }\n",
    "\n",
    "        print(\"Done training\")\n",
    "        print(\"\")\n",
    "\n",
    "    # Dump data\n",
    "    utils.pickle_dump(list_of_param, CONFIG_DATA['list_of_param_path'])\n",
    "    utils.pickle_dump(list_of_model, CONFIG_DATA['list_of_model_path'])\n",
    "    utils.pickle_dump(list_of_tuned_model, CONFIG_DATA['list_of_tuned_model_path'])\n",
    "\n",
    "    if return_file:\n",
    "        return list_of_param, list_of_model, list_of_tuned_model    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model objects\n",
      "Training model : LogisticRegression\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV 1/5; 1/5] START C=0.01, max_iter=500, penalty=l2............................\n",
      "[CV 1/5; 1/5] END C=0.01, max_iter=500, penalty=l2;, score=0.857 total time=   0.1s\n",
      "[CV 2/5; 1/5] START C=0.01, max_iter=500, penalty=l2............................\n",
      "[CV 2/5; 1/5] END C=0.01, max_iter=500, penalty=l2;, score=0.854 total time=   0.0s\n",
      "[CV 3/5; 1/5] START C=0.01, max_iter=500, penalty=l2............................\n",
      "[CV 3/5; 1/5] END C=0.01, max_iter=500, penalty=l2;, score=0.847 total time=   0.0s\n",
      "[CV 4/5; 1/5] START C=0.01, max_iter=500, penalty=l2............................\n",
      "[CV 4/5; 1/5] END C=0.01, max_iter=500, penalty=l2;, score=0.849 total time=   0.0s\n",
      "[CV 5/5; 1/5] START C=0.01, max_iter=500, penalty=l2............................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/5] END C=0.01, max_iter=500, penalty=l2;, score=0.854 total time=   0.0s\n",
      "[CV 1/5; 2/5] START C=0.01, max_iter=100, penalty=l1............................\n",
      "[CV 1/5; 2/5] END C=0.01, max_iter=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/5; 2/5] START C=0.01, max_iter=100, penalty=l1............................\n",
      "[CV 2/5; 2/5] END C=0.01, max_iter=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/5; 2/5] START C=0.01, max_iter=100, penalty=l1............................\n",
      "[CV 3/5; 2/5] END C=0.01, max_iter=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/5; 2/5] START C=0.01, max_iter=100, penalty=l1............................\n",
      "[CV 4/5; 2/5] END C=0.01, max_iter=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 5/5; 2/5] START C=0.01, max_iter=100, penalty=l1............................\n",
      "[CV 5/5; 2/5] END C=0.01, max_iter=100, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/5; 3/5] START C=0.01, max_iter=500, penalty=l1............................\n",
      "[CV 1/5; 3/5] END C=0.01, max_iter=500, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/5; 3/5] START C=0.01, max_iter=500, penalty=l1............................\n",
      "[CV 2/5; 3/5] END C=0.01, max_iter=500, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/5; 3/5] START C=0.01, max_iter=500, penalty=l1............................\n",
      "[CV 3/5; 3/5] END C=0.01, max_iter=500, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/5; 3/5] START C=0.01, max_iter=500, penalty=l1............................\n",
      "[CV 4/5; 3/5] END C=0.01, max_iter=500, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 5/5; 3/5] START C=0.01, max_iter=500, penalty=l1............................\n",
      "[CV 5/5; 3/5] END C=0.01, max_iter=500, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 1/5; 4/5] START C=0.1, max_iter=300, penalty=l2.............................\n",
      "[CV 1/5; 4/5] END C=0.1, max_iter=300, penalty=l2;, score=0.856 total time=   0.1s\n",
      "[CV 2/5; 4/5] START C=0.1, max_iter=300, penalty=l2.............................\n",
      "[CV 2/5; 4/5] END C=0.1, max_iter=300, penalty=l2;, score=0.854 total time=   0.0s\n",
      "[CV 3/5; 4/5] START C=0.1, max_iter=300, penalty=l2.............................\n",
      "[CV 3/5; 4/5] END C=0.1, max_iter=300, penalty=l2;, score=0.848 total time=   0.0s\n",
      "[CV 4/5; 4/5] START C=0.1, max_iter=300, penalty=l2.............................\n",
      "[CV 4/5; 4/5] END C=0.1, max_iter=300, penalty=l2;, score=0.849 total time=   0.0s\n",
      "[CV 5/5; 4/5] START C=0.1, max_iter=300, penalty=l2.............................\n",
      "[CV 5/5; 4/5] END C=0.1, max_iter=300, penalty=l2;, score=0.855 total time=   0.0s\n",
      "[CV 1/5; 5/5] START C=0.1, max_iter=300, penalty=l1.............................\n",
      "[CV 1/5; 5/5] END C=0.1, max_iter=300, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 2/5; 5/5] START C=0.1, max_iter=300, penalty=l1.............................\n",
      "[CV 2/5; 5/5] END C=0.1, max_iter=300, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 3/5; 5/5] START C=0.1, max_iter=300, penalty=l1.............................\n",
      "[CV 3/5; 5/5] END C=0.1, max_iter=300, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 4/5; 5/5] START C=0.1, max_iter=300, penalty=l1.............................\n",
      "[CV 4/5; 5/5] END C=0.1, max_iter=300, penalty=l1;, score=nan total time=   0.0s\n",
      "[CV 5/5; 5/5] START C=0.1, max_iter=300, penalty=l1.............................\n",
      "[CV 5/5; 5/5] END C=0.1, max_iter=300, penalty=l1;, score=nan total time=   0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cahya_pacmann/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "15 fits failed out of a total of 25.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cahya_pacmann/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/cahya_pacmann/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/home/cahya_pacmann/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/cahya_pacmann/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.85223008        nan        nan 0.85225084        nan]\n",
      "  warnings.warn(\n",
      "/home/cahya_pacmann/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 3 is smaller than n_iter=5. Running 3 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "\n",
      "Training model : XGBClassifier\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "[CV 1/5; 1/3] START n_estimators=50.............................................\n",
      "[CV 1/5; 1/3] END ..............n_estimators=50;, score=0.855 total time=   0.4s\n",
      "[CV 2/5; 1/3] START n_estimators=50.............................................\n",
      "[CV 2/5; 1/3] END ..............n_estimators=50;, score=0.845 total time=   0.4s\n",
      "[CV 3/5; 1/3] START n_estimators=50.............................................\n",
      "[CV 3/5; 1/3] END ..............n_estimators=50;, score=0.845 total time=   0.3s\n",
      "[CV 4/5; 1/3] START n_estimators=50.............................................\n",
      "[CV 4/5; 1/3] END ..............n_estimators=50;, score=0.845 total time=   0.3s\n",
      "[CV 5/5; 1/3] START n_estimators=50.............................................\n",
      "[CV 5/5; 1/3] END ..............n_estimators=50;, score=0.850 total time=   0.4s\n",
      "[CV 1/5; 2/3] START n_estimators=100............................................\n",
      "[CV 1/5; 2/3] END .............n_estimators=100;, score=0.849 total time=   1.2s\n",
      "[CV 2/5; 2/3] START n_estimators=100............................................\n",
      "[CV 2/5; 2/3] END .............n_estimators=100;, score=0.835 total time=   1.1s\n",
      "[CV 3/5; 2/3] START n_estimators=100............................................\n",
      "[CV 3/5; 2/3] END .............n_estimators=100;, score=0.839 total time=   1.0s\n",
      "[CV 4/5; 2/3] START n_estimators=100............................................\n",
      "[CV 4/5; 2/3] END .............n_estimators=100;, score=0.839 total time=   1.0s\n",
      "[CV 5/5; 2/3] START n_estimators=100............................................\n",
      "[CV 5/5; 2/3] END .............n_estimators=100;, score=0.842 total time=   1.5s\n",
      "[CV 1/5; 3/3] START n_estimators=200............................................\n",
      "[CV 1/5; 3/3] END .............n_estimators=200;, score=0.839 total time=   3.0s\n",
      "[CV 2/5; 3/3] START n_estimators=200............................................\n",
      "[CV 2/5; 3/3] END .............n_estimators=200;, score=0.826 total time=   4.2s\n",
      "[CV 3/5; 3/3] START n_estimators=200............................................\n",
      "[CV 3/5; 3/3] END .............n_estimators=200;, score=0.833 total time=   2.1s\n",
      "[CV 4/5; 3/3] START n_estimators=200............................................\n",
      "[CV 4/5; 3/3] END .............n_estimators=200;, score=0.829 total time=   2.6s\n",
      "[CV 5/5; 3/3] START n_estimators=200............................................\n",
      "[CV 5/5; 3/3] END .............n_estimators=200;, score=0.834 total time=   3.2s\n",
      "Done training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_param, list_of_model, list_of_tuned_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {'model': RandomizedSearchCV(cv=5, estimator=LogisticRegression(), n_iter=5, n_jobs=1,\n",
       "                     param_distributions={'C': [0.01, 0.1],\n",
       "                                          'max_iter': [100, 300, 500],\n",
       "                                          'penalty': ['l1', 'l2']},\n",
       "                     random_state=123, scoring='roc_auc', verbose=10),\n",
       "  'train_auc': 0.8525712625980069,\n",
       "  'valid_auc': 0.8551535886466674,\n",
       "  'best_params': {'penalty': 'l2', 'max_iter': 300, 'C': 0.1}},\n",
       " 'XGBClassifier': {'model': RandomizedSearchCV(cv=5,\n",
       "                     estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                             callbacks=None,\n",
       "                                             colsample_bylevel=None,\n",
       "                                             colsample_bynode=None,\n",
       "                                             colsample_bytree=None,\n",
       "                                             early_stopping_rounds=None,\n",
       "                                             enable_categorical=False,\n",
       "                                             eval_metric=None, feature_types=None,\n",
       "                                             gamma=None, gpu_id=None,\n",
       "                                             grow_policy=None,\n",
       "                                             importance_type=None,\n",
       "                                             interaction_constraints=None,\n",
       "                                             learning_rate...\n",
       "                                             max_cat_threshold=None,\n",
       "                                             max_cat_to_onehot=None,\n",
       "                                             max_delta_step=None, max_depth=None,\n",
       "                                             max_leaves=None,\n",
       "                                             min_child_weight=None, missing=nan,\n",
       "                                             monotone_constraints=None,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             num_parallel_tree=None,\n",
       "                                             predictor=None, random_state=None, ...),\n",
       "                     n_iter=5, n_jobs=1,\n",
       "                     param_distributions={'n_estimators': [50, 100, 200]},\n",
       "                     random_state=123, scoring='roc_auc', verbose=10),\n",
       "  'train_auc': 0.9186069705845513,\n",
       "  'valid_auc': 0.8605813878486432,\n",
       "  'best_params': {'n_estimators': 50}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_tuned_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(return_file=True):\n",
    "    \"\"\"Function to get the best model\"\"\"\n",
    "    # Load tuned model\n",
    "    list_of_tuned_model = utils.pickle_load(CONFIG_DATA['list_of_tuned_model_path'])\n",
    "\n",
    "    # Get the best model\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_performance = -99999\n",
    "    best_model_param = None\n",
    "\n",
    "    for model_name, model in list_of_tuned_model.items():\n",
    "        if model['valid_auc'] > best_performance:\n",
    "            best_model_name = model_name\n",
    "            best_model = model['model']\n",
    "            best_performance = model['valid_auc']\n",
    "            best_model_param = model['best_params']\n",
    "\n",
    "    # Dump the best model\n",
    "    utils.pickle_dump(best_model, CONFIG_DATA['best_model_path'])\n",
    "\n",
    "    # Print\n",
    "    print('=============================================')\n",
    "    print('Best model        :', best_model_name)\n",
    "    print('Metric score      :', best_performance)\n",
    "    print('Best model params :', best_model_param)\n",
    "    print('=============================================')\n",
    "\n",
    "    if return_file:\n",
    "        return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Best model        : XGBClassifier\n",
      "Metric score      : 0.8605813878486432\n",
      "Best model params : {'n_estimators': 50}\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "best_model = get_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, feature_types=None,\n",
       "                                           gamma=None, gpu_id=None,\n",
       "                                           grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate...\n",
       "                                           max_cat_threshold=None,\n",
       "                                           max_cat_to_onehot=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           max_leaves=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None, ...),\n",
       "                   n_iter=5, n_jobs=1,\n",
       "                   param_distributions={&#x27;n_estimators&#x27;: [50, 100, 200]},\n",
       "                   random_state=123, scoring=&#x27;roc_auc&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, feature_types=None,\n",
       "                                           gamma=None, gpu_id=None,\n",
       "                                           grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate...\n",
       "                                           max_cat_threshold=None,\n",
       "                                           max_cat_to_onehot=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           max_leaves=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None, ...),\n",
       "                   n_iter=5, n_jobs=1,\n",
       "                   param_distributions={&#x27;n_estimators&#x27;: [50, 100, 200]},\n",
       "                   random_state=123, scoring=&#x27;roc_auc&#x27;, verbose=10)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                           callbacks=None,\n",
       "                                           colsample_bylevel=None,\n",
       "                                           colsample_bynode=None,\n",
       "                                           colsample_bytree=None,\n",
       "                                           early_stopping_rounds=None,\n",
       "                                           enable_categorical=False,\n",
       "                                           eval_metric=None, feature_types=None,\n",
       "                                           gamma=None, gpu_id=None,\n",
       "                                           grow_policy=None,\n",
       "                                           importance_type=None,\n",
       "                                           interaction_constraints=None,\n",
       "                                           learning_rate...\n",
       "                                           max_cat_threshold=None,\n",
       "                                           max_cat_to_onehot=None,\n",
       "                                           max_delta_step=None, max_depth=None,\n",
       "                                           max_leaves=None,\n",
       "                                           min_child_weight=None, missing=nan,\n",
       "                                           monotone_constraints=None,\n",
       "                                           n_estimators=100, n_jobs=None,\n",
       "                                           num_parallel_tree=None,\n",
       "                                           predictor=None, random_state=None, ...),\n",
       "                   n_iter=5, n_jobs=1,\n",
       "                   param_distributions={'n_estimators': [50, 100, 200]},\n",
       "                   random_state=123, scoring='roc_auc', verbose=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = np.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold(return_file=True):\n",
    "    \"\"\"Function to tune & get the best decision threshold\"\"\"\n",
    "    # Load data & model\n",
    "    X_valid = utils.pickle_load(CONFIG_DATA['valid_clean_path'][0])\n",
    "    y_valid = utils.pickle_load(CONFIG_DATA['valid_clean_path'][1])\n",
    "    best_model = utils.pickle_load(CONFIG_DATA['best_model_path'])\n",
    "\n",
    "    # Get the proba pred\n",
    "    y_pred_proba = best_model.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "    # Initialize\n",
    "    metric_threshold = pd.Series([])\n",
    "    \n",
    "    # Optimize\n",
    "    for threshold_value in THRESHOLD:\n",
    "        # Get predictions\n",
    "        y_pred = (y_pred_proba >= threshold_value).astype(int)\n",
    "\n",
    "        # Get the F1 score\n",
    "        metric_score = f1_score(y_valid, y_pred, average='macro')\n",
    "\n",
    "        # Add to the storage\n",
    "        metric_threshold[metric_score] = threshold_value\n",
    "\n",
    "    # Find the threshold @max metric score\n",
    "    metric_score_max_index = metric_threshold.index.max()\n",
    "    best_threshold = metric_threshold[metric_score_max_index]\n",
    "    print('=============================================')\n",
    "    print('Best threshold :', best_threshold)\n",
    "    print('Metric score   :', metric_score_max_index)\n",
    "    print('=============================================')\n",
    "    \n",
    "    # Dump file\n",
    "    utils.pickle_dump(best_threshold, CONFIG_DATA['best_threshold_path'])\n",
    "\n",
    "    if return_file:\n",
    "        return best_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1809/1872309259.py:12: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  metric_threshold = pd.Series([])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "Best threshold : 0.8585858585858587\n",
      "Metric score   : 0.695584290568394\n",
      "=============================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8585858585858587"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_threshold()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
